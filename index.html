<!DOCTYPE html>
<html>
    <head>
        <title>Multimodal Hashtag Generator</title>
        <!-- link to main stylesheet -->
        <link rel="stylesheet" type="text/css" href="css/main.css">
    </head>
    <body>
        <div class="container">
        
        <div class="blurb">
    <div class="title-project" style="text-align: center">
        <h1>Multimodal image/video based hashtag generator</h1>
        <h4 style="font-style: italic"> (Abhishek Mathur, Shashank Srikant, Anant Agrawal and Vatsal Soni)</h4><br><br><br><br>
    </div>
    <div class="teaser" style="text-align: center">
        <!-- <img src="/images/teaser.png" alt="Mountain View" style="width:760px;height:280px;"> -->
        <!-- <video width="500" height="280" controls> -->
            <!-- <source src="/videos/project_video.mp4" type="video/mp4" frameborder="0" allowfullscreen> -->
        <!-- </video> -->
        <iframe width="560" height="315" src="https://www.youtube.com/embed/mc6xvyuNYE0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>
    <div class="content">
        <!-- <p style="font-size: 18px; text-align: justify;">
                <span style="font-weight: bold;" size="10"><br><br><br><br><b><h2>Abstract</h2></b></span> The goal of this project is to generate hashtags given a multi-modal post on OSM like Instagram that may contain both images and text content. The user might also provide a set of few seed hashtags as input and the generated hashtags should be relevant to these hashtags as well. In
        the Figure 1, we are given an image, text content and certain hashtags (#dog, #birthday) as the inputs 1 . Our proposed method needs to utilize this data and suggest few other hashtags like
        #celebration, #party or #puppy for the same post. The project thus requires us to leverage the latest methods in computer vision such as Convolutional Neural Networks (Resnet50, Transfer
        learning) and in NLP such as glove embeddings, and word2vec to get a high precision/recall on the given task. We propose a two level hierarchical system where the predictions made by an image
        classifier are subsequently used by the corresponding text based approach.</p> -->
        <br><br><br><br>
        <hr>
        <hr>
        <!-- <h4> arXiv Link of the paper can be found <a href="https://arxiv.org/abs/1903.10641">here</a>.</h4> -->
        
        <h2><span style="font-weight: bold;">Abstract</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            The goal of this project is to generate hashtags given a multi-modal post on OSM (Online Social
Media) like Instagram that may contain both images and text content. The user might also provide
a set of few seed hashtags as input and the generated hashtags should be relevant to these hashtags
as well. In the Figure 1, we are given an image, text content and certain hashtags (#dog, #birthday)
as the inputs 1 . Our proposed method needs to utilize this data and suggest few other hashtags
like #celebration, #party or #puppy for the same post. The project thus requires us to leverage
the latest methods in computer vision such as Convolutional Neural Networks (Resnet50, Transfer
learning) and in NLP such as glove embeddings, and word2vec to get a high precision/recall on the
given task. We propose a two level hierarchical system where the predictions made by an image
classifier are subsequently used by the corresponding text based approach. Our proposed approach
shows significantly better results than the proposed baselines. To foster further research and to
encourage reproducibility we release all our code and dataset respectively in our project page.
        </h4><br><br>
        <center><img src="fig1.jpg" width=20%><br><h5>Figure 1: Predicting the correct hashtag: #dog is difficult without the image</h5><br></center>
        <hr>
        <hr>

        <h2><span style="font-weight: bold;">Related Work</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            The task of hashtag generation has been studied a lot in the past. One set of approaches use only
the text data to predict relevant hashtags. Most of these approaches model
the problem as either a classification or ranking task. Another set of approaches involve the use
of topic modelling based methods such as LDA to generate relevant hashtags. LDA is often used to
model the underlying topic assignment of language classified tweets.  The LDA
based models are improved by using a novel tweet pooling methods. However, none of these methods make use of multimodal data such as both images and text for the task of hashtag recommendation / generation
which is what we aim to solve.
There also exist works which predict the hastags using the images only. Zero-shot learning is sometimes used along with a novel loss function to predict the hashtags for a given image.
On the other hand, a scene model is often used and an object model trained on the MIT Places dataset
and Imagenet dataset respectively to predict multiple hashtags for a given image. They use a binary
cross entropy loss to train their problem in the multi-label setting. None of these approaches utilize
multimodal data and as the problem is posed as a classification task, these approaches are not
capable of generating new hashtags that are not a label in the dataset.
Recently, a few approaches have also been proposed that use multimodal data for image
captioning and hashtag recommendation. A novel co-attention mechanism is used to first
predict relevant hashtags and then generate personalized recommendations. A few other approaches also deal with the task of hashtag generation / recommendation using
multimodal data. However, unlike these methods, our approach is trained on a few specific topics
of Instagram posts and is capable of generating a diverse set of relevant hashtags.
        </h4>


        <hr>
        <hr>


        <h2><span style="font-weight: bold;">Dataset details</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            A few datasets such as YFCC100M and HARRISON dataset deal with the task of hashtag
generation using images. However, these datasets do not provide us with text data that is required
for the training of our multimodal system. A few other datasets such as deal with the task of
personalized image captioning given text and image data along with prior user posts.
Thus, we collect our own dataset for the given task by scraping posts from Instagram using
Selenium as the official Instagram API has rate limits. Our dataset consists of hashtags collected
from a diverse set of few topics mentioned below:<br><br>
<center><img src="data.jpg" width=80%><br><h5>Figure 2: A brief description of our data collection pipeline</h5></center>
<ul>
    <li>Pets</li>
    <li>Art</li>
    <li>Jewellery</li>
    <li>Food</li>
    <li>Architecture</li>
    <li>Babies</li>
    <li>Nature</li>
    <li>Travel</li>
</ul><br>For the purposes of training and evaluating
our model, we split our data into a train, validation and test set with 70%, 15% and 15% of the
data respectively.<br><i>Description:</i><br>
        <h3><a href="https://drive.google.com/drive/folders/1IakHwyTgaTKxZa0RqQ99TRuamhEi-8xp?usp=sharing ">Link to the datasets</a></h3><br>
        </h4><hr>
        <hr>
        <h2><span style="font-weight: bold;">Image Classifier</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            The image classifier to be used will be a Resnet50 model that gave very high accuracy in the
Imagenet challenge. We intend to leverage transfer learning to finetune the model on our dataset.
Only the last layer of the model will be trained and we won’t perform backpropagation on the
remaining layers of the network. The loss function used will be Binary Cross Entropy loss or the
normal cross entropy loss depending on whether it is multi-label or single label tasks respectively.
As of now, a Resnet50 [2] model with Binary cross entropy loss was trained to do multilabel
predictions. However, the model isn’t able to generalize well for the given data and the same model
will also be tested for single label topic prediction.
        </h4><br>
        <hr>
        <hr>
        <h2><span style="font-weight: bold;">Text Classifier</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            We have trained a word embedding approach like Word2Vec [3] and Glove embeddings [5] on our
corpus and used distance measures such as cosine similarity to get the most relevant hashtags. As
one of the baselines, we also aim to implement a simple topic modelling based approach (LDA) for
the task of hashtag generation. The approach is detailed below.
A sample post representing the typical posts on Instagram has been shown in Figure 2.
        </h4><br><br>
        <center><img src="fig2.jpg" width=20%><br><h5>Figure 2: Shows lack of sufficient text</h5></center>
        <h4 style="text-align: justify; font-weight: normal;">Typically a post consists of the following:
<ul><li>A photo</li>
<li>A caption of 2-3 lines</li>
<li>A bunch of hashtags</li>
</ul><br>
    From above it is clear that the amount of text present is very less and therefore using only
the text for training our model won’t be enough. We tackle the above problem using below 2
approaches:
<ul>
<li>Assuming the hashtags that appear together are semantically related to each other we can also
convert them to text and include them in our training corpus. Using the above assumption
we prepare our corpus by first preprocessing (removing non-english texts, emoticons, etc.)
and then training two different models on the prepared corpus.
<ul><li>Word2Vec: Using 30 epochs and window size of 5 we trained the model to create 300
dimensional word embeddings.</li>
<li>GloVe: Using 30 epochs for training we trained the model to create 300 dimensional
word embeddings.</li></ul></li>
<li>To better capture the semantic relationship between words we use pre-trained word embed-
dings for a corpus size of 400K words trained on wikipedia data. We train our word2Vec
model with our corpus on top of it which helps to capture the context of the instagram
corpus. Therefore our vocabulary is able to cover more words and is able to capture both
the generic and insta-specific meaning of a word.</li>
</ul>
    </h4>
    <hr>
    <hr>
    <h2><span style="font-weight: bold;">Combining both approaches</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            We have trained the image classifier to predict the category to which a post belongs (10 categories
mentioned above), and trained 10 different embeddings for each individual topic. We have used glove for these word embeddings. During test time, given a new post with an image, the image
classifier classifies into one of the 10 categories and then the embedding for that particular
category is used to generate the new hashtags. In case no text is provided, we simply
return the top K hashtags for the topic predicted by the classifier.
        </h4><br>
        <hr>
        <hr>
    <h2><span style="font-weight: bold;">Implementations and Code</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            The repo contains all our
data collection scripts (Instagram scraper), image classifier code and the code for Word2Vec and
Glove embedding training.
The text based approaches perform well for our given data and will be complemented greatly
if given additional information by the image classifier as well. Some sample outputs of the text
based approach are given in Figure 3 and 4.
        </h4>
            <h3><a href="https://github.com/talsperre/IRE-hashtag-generation">Link to the repository (Github)</a><br></h3>
        <center><img src="fig3.jpg" width=20%><br><h5>Figure 3: Predicted hashtags: #ig #amazing #life #beauty #inspiration</h5></center><hr width=30%><br>
        <center><img src="fig4.jpg" width=20%><br><h5>Figure 4: Predicted hashtags: #hiking #mountain #adventure #forest #outdoor</h5></center>
        <hr>
        <hr>
        <hr>


        <h2><span style="font-weight: bold;">References</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            <ol>
                <li>Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim. Attend to you: Personalized image
captioning with context sequence memory networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 895–903, 2017.</li>
                <li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.</li>
                <li>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word
representations in vector space. arXiv preprint arXiv:1301.3781, 2013.</li>
                <li>Minseok Park, Hanxiang Li, and Junmo Kim. Harrison: A benchmark on hashtag recommen-
dation for real-world images in social networks, 2016.</li>
                <li>Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pages 1532–1543, 2014.</li>
                <li>Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas
Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research.
arXiv preprint arXiv:1503.01817, 2015.</li>
                <li>Qi Zhang, Jiawen Wang, Haoran Huang, Xuanjing Huang, and Yeyun Gong. Hashtag recom-
mendation for multimodal microblog using co-attention network. In IJCAI, pages 3420–3426,
2017.</li>
            </ol>
        </h4>
        <hr>



    </div>



    <!-- <div class="qual-results">
        <h3 style="font-weight: bold;">Qualitative Results</h3>
        <h4 style="font-weight: bold;">Sample Video sequence in KITTI Tracking dataset</h4>
        <video width="500" height="280" controls>
            <source src="/videos/infer-kitti-no-audio.mp4" type="video/mp4" frameborder="0" allowfullscreen>
        </video>
        <h4 style="font-weight: bold;">Sample transfer in Cityscapes dataset</h4>
        <video width="500" height="280" controls>
            <source src="/videos/infer-cscapes-no-audio.mp4" type="video/mp4" frameborder="0" allowfullscreen>
        </video>
        <h4 style="font-weight: bold;">Sample transfer in Oxford RobotCar dataset</h4>
        <video width="500" height="280" controls>
            <source src="/videos/infer-robotcar-no-audio.mp4" type="video/mp4" frameborder="0" allowfullscreen>
        </video>
        <h4 style="font-weight: bold;">Trajectories plotted on the Occupancy Grids for KITTI dataset</h4>
        <img src="/images/qual_kitti.png" style="width:1260px;height:280px;">
    </div>    
    <hr>
    <h3 style="font-weight: bold;">Some more qualitative plots</h4>
    <div class="image-container">
        <div class="imgContainer">
            <img src="/images/val-cv-1-0007.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-2-0001.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-3-0008.png" height="240" width="320"/>
        </div>
        <div>
            <img src="/images/val-cv-3-0009.png" height="240" width="320"/>
       </div>       
    </div>
    <br> 
    <div class="image-container">
        <div class="imgContainer">
            <img src="/images/val-cv-0-0013.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-1-0006.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-4-0010.png" height="240" width="320"/>
        </div>
        <div>
            <img src="/images/val-cv-1-0004.png" height="240" width="320"/>
       </div>       
    </div>
    <br> 
    <hr>
    <div class="results">
        <h3 style="font-weight: bold;">Results</h3>
        <p style="text-align:justify; font-size: 14px;">We perform 5 fold cross validation while training and evaluating on KITTI. The error for each split is given in the below 
            tables. We report the pixel loss (L2 norm) here and to get the error in m, multiply the values by 0.25. The average loss for a given model is computed as the 
            weighted mean of the losses of each individual split. The weights used here are the number of sequences in each split.
        </p>
        <h4 style="font-weight: bold;">INFER-Skip results with all channels (pixel loss)</h4>
        <span style="text-align: center;">
            <table>
                    <tr>
                        <th></th>
                        <th>1s</th>
                        <th>2s</th>
                        <th>3s</th>
                        <th>4s</th>
                        <th>Num Sequences</th>
                    </tr>
                    <tr>
                        <td>Split 0</td>
                        <td>2.2</td>
                        <td>2.91</td>
                        <td>3.68</td>
                        <td>5.29</td>
                        <td>16</td>
                    </tr>
                    <tr>
                        <td>Split 1</td>
                        <td>1.97</td>
                        <td>2.72</td>
                        <td>2.75</td>
                        <td>2.75</td>
                        <td>19</td>
                    </tr>
                    <tr>
                        <td>Split 2</td>
                        <td>2.54</td>
                        <td>3.3</td>
                        <td>4.15</td>
                        <td>5.74</td>
                        <td>28</td>
                    </tr>
                    <tr>
                        <td>Split 3</td>
                        <td>2.32</td>
                        <td>3.16</td>
                        <td>3.86</td>
                        <td>4.82</td>
                        <td>22</td>
                    </tr>
                    <tr>
                        <td>Split 4</td>
                        <td>2.06</td>
                        <td>2.9</td>
                        <td>4.13</td>
                        <td>5.79</td>
                        <td>16</td>
                    </tr>                    
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">INFER results with all channels (pixel loss)</h4>        
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>2.82</td>
                    <td>3.8</td>
                    <td>5.56</td>
                    <td>7.9</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>2.57</td>
                    <td>3.55</td>
                    <td>4.31</td>
                    <td>4.76</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.4</td>
                    <td>3.98</td>
                    <td>5.64</td>
                    <td>7.75</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.99</td>
                    <td>3.94</td>
                    <td>4.49</td>
                    <td>5.43</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>1.44</td>
                    <td>1.7</td>
                    <td>2.65</td>
                    <td>4.19</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">Baseline (pixel loss)</h4>        
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>3.17</td>
                    <td>5.04</td>
                    <td>6.47</td>
                    <td>8.16</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>1.66</td>
                    <td>2.80</td>
                    <td>3.71</td>
                    <td>4.55</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>4.07</td>
                    <td>7.45</td>
                    <td>9.68</td>
                    <td>11.13</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>3.50</td>
                    <td>5.08</td>
                    <td>6.29</td>
                    <td>7.67</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>2.32</td>
                    <td>2.75</td>
                    <td>4.16</td>
                    <td>6.15</td>
                    <td>16</td>
                </tr>
            </table>
        </span>        
        <br>
        <h4 style="font-weight: bold;">INFER w/o road (pixel loss)</h4>
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>3.08</td>
                    <td>4.68</td>
                    <td>6.03</td>
                    <td>8.23</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>3.76</td>
                    <td>6.08</td>
                    <td>9.73</td>
                    <td>13.83</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.65</td>
                    <td>4.76</td>
                    <td>6.94</td>
                    <td>9.21</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.32</td>
                    <td>4.16</td>
                    <td>6.49</td>
                    <td>9.18</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>2.53</td>
                    <td>4.58</td>
                    <td>6.84</td>
                    <td>9.61</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">INFER w/o lane (pixel loss)</h4>
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>2.34</td>
                    <td>3.42</td>
                    <td>4.85</td>
                    <td>7.32</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>1.73</td>
                    <td>2.42</td>
                    <td>2.81</td>
                    <td>3.23</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.97</td>
                    <td>3.76</td>
                    <td>4.46</td>
                    <td>5.5</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.66</td>
                    <td>3.67</td>
                    <td>4.27</td>
                    <td>5.05</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>1.29</td>
                    <td>1.4</td>
                    <td>2.01</td>
                    <td>3.06</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">INFER w/o obstacles (pixel loss)</h4>
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>2.08</td>
                    <td>2.85</td>
                    <td>3.55</td>
                    <td>4.48</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>2.42</td>
                    <td>3.25</td>
                    <td>3.38</td>
                    <td>3.23</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.20</td>
                    <td>3.47</td>
                    <td>4.56</td>
                    <td>5.56</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.65</td>
                    <td>3.84></td>
                    <td>4.75</td>
                    <td>6.03</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>1.39</td>
                    <td>2.23</td>
                    <td>3.42</td>
                    <td>5.17</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>        
    </div> -->
</div><!-- /.blurb -->

        
        </div><!-- /.container -->
        <!-- <footer>
            <ul>
                <li>Email: <a href="mailto:s.shashank2401@gmail.com">s.shashank2401@gmail.com</a></li>
                <li><a href="https://github.com/talsperre">github.com/talsperre</a></li>
            </ul>
        </footer>                            -->
    </body>
</html>