<!DOCTYPE html>
<html>
    <head>
        <title>Multimodal Hashtag Generator</title>
        <!-- link to main stylesheet -->
        <link rel="stylesheet" type="text/css" href="css/main.css">
    </head>
    <body>
        <div class="navbar"><br><br>
            <button class="tablink1"">Links</button><br><br>
            <a href="https://drive.google.com/drive/folders/1IakHwyTgaTKxZa0RqQ99TRuamhEi-8xp?usp=sharing" ><button class="tablink"">Dataset</button></a><br>
            <a href="https://github.com/talsperre/IRE-hashtag-generation" ><button class="tablink">Repository</button></a><br>
            <a href="https://rebrand.ly/ire-hashtags-generation-video" ><button class="tablink">Video</button></a>
        </div>
        <div class="container">
        
        <div class="blurb">
    <div class="title-project" style="text-align: center"><hr><hr>
        <h1>Multimodal image/video based hashtag generator</h1>
        <h4 style="font-style: italic"> (Abhishek Mathur, Shashank Srikant, Anant Agrawal and Vatsal Soni)</h4><br><br><br><br>
    </div>

    <div class="teaser" style="text-align: center">
        <!-- <img src="/images/teaser.png" alt="Mountain View" style="width:760px;height:280px;"> -->
        <!-- <video width="500" height="280" controls> -->
            <source src="/videos/project_video.mp4" type="video/mp4" frameborder="0" allowfullscreen>
        <!-- </video> -->
        <!-- <iframe width="560" height="315" src="https://rebrand.ly/ire-hashtags-generation-video" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    </div>

    <div class="content">
        <!-- <p style="font-size: 18px; text-align: justify;">
                <span style="font-weight: bold;" size="10"><br><br><br><br><b><h2>Abstract</h2></b></span> The goal of this project is to generate hashtags given a multi-modal post on OSM like Instagram that may contain both images and text content. The user might also provide a set of few seed hashtags as input and the generated hashtags should be relevant to these hashtags as well. In
        the Figure 1, we are given an image, text content and certain hashtags (#dog, #birthday) as the inputs 1 . Our proposed method needs to utilize this data and suggest few other hashtags like
        #celebration, #party or #puppy for the same post. The project thus requires us to leverage the latest methods in computer vision such as Convolutional Neural Networks (Resnet50, Transfer
        learning) and in NLP such as glove embeddings, and word2vec to get a high precision/recall on the given task. We propose a two level hierarchical system where the predictions made by an image
        classifier are subsequently used by the corresponding text based approach.</p> -->
        <!-- <br><br><br><br> -->
        <br><br>
        <!-- <h4> arXiv Link of the paper can be found <a href="https://arxiv.org/abs/1903.10641">here</a>.</h4> -->
        
        <h2><span style="font-weight: bold;">Abstract</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            The goal of this project is to generate hashtags given a multi-modal post on OSM (Online Social
Media) like Instagram that may contain both images and text content. The user might also provide
a set of few seed hashtags as input and the generated hashtags should be relevant to these hashtags
as well. In the Figure 1, we are given an image, text content and certain hashtags (#dog, #birthday)
as the inputs 1 . Our proposed method needs to utilize this data and suggest few other hashtags
like #celebration, #party or #puppy for the same post. The project thus requires us to leverage
the latest methods in computer vision such as Convolutional Neural Networks (Resnet50, Transfer
learning) and in NLP such as glove embeddings, and word2vec to get a high precision/recall on the
given task. We propose a two level hierarchical system where the predictions made by an image
classifier are subsequently used by the corresponding text based approach. Our proposed approach
shows significantly better results than the proposed baselines. To foster further research and to
encourage reproducibility we release all our code and dataset respectively in our project page.
        </h4><br><br>
        <center><img src="fig1.jpg" width=30%><br><h5>Figure 1: Predicting the correct hashtag: #dog is difficult without the image</h5><br></center>
        <hr>
        <hr>
        <br>
        <h2><span style="font-weight: bold;">Related Work</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            The task of hashtag generation has been studied a lot in the past. One set of approaches use only
the text data to predict relevant hashtags. Most of these approaches model
the problem as either a classification or ranking task. Another set of approaches involve the use
of topic modelling based methods such as LDA to generate relevant hashtags. LDA is often used to
model the underlying topic assignment of language classified tweets.  The LDA
based models are improved by using a novel tweet pooling methods. However, none of these methods make use of multimodal data such as both images and text for the task of hashtag recommendation / generation
which is what we aim to solve.
There also exist works which predict the hastags using the images only. Zero-shot learning is sometimes used along with a novel loss function to predict the hashtags for a given image.
On the other hand, a scene model is often used and an object model trained on the MIT Places dataset
and Imagenet dataset respectively to predict multiple hashtags for a given image. They use a binary
cross entropy loss to train their problem in the multi-label setting. None of these approaches utilize
multimodal data and as the problem is posed as a classification task, these approaches are not
capable of generating new hashtags that are not a label in the dataset.
Recently, a few approaches have also been proposed that use multimodal data for image
captioning and hashtag recommendation. A novel co-attention mechanism is used to first
predict relevant hashtags and then generate personalized recommendations. A few other approaches also deal with the task of hashtag generation / recommendation using
multimodal data. However, unlike these methods, our approach is trained on a few specific topics
of Instagram posts and is capable of generating a diverse set of relevant hashtags.
        </h4>


        <br>
        <hr>
        <hr>
        <br>

        <h2><span style="font-weight: bold;">Dataset details</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            A few datasets such as YFCC100M and HARRISON dataset deal with the task of hashtag
generation using images. However, these datasets do not provide us with text data that is required
for the training of our multimodal system. A few other datasets such as deal with the task of
personalized image captioning given text and image data along with prior user posts.
Thus, we collect our own dataset for the given task by scraping posts from Instagram using
Selenium as the official Instagram API has rate limits. Our dataset consists of hashtags collected
from a diverse set of few topics mentioned below:<br><br>
<center><img src="data.jpg" width=80%><br><h5>Figure 2: A brief description of our data collection pipeline</h5></center>
<ul>
    <li>Pets</li>
    <li>Art</li>
    <li>Jewellery</li>
    <li>Food</li>
    <li>Architecture</li>
    <li>Babies</li>
    <li>Nature</li>
    <li>Travel</li>
</ul><br>For the purposes of training and evaluating
our model, we split our data into a train, validation and test set with 70%, 15% and 15% of the
data respectively.<br><br>
                <table>
                    <tr>
                        <th>Description</th>
                        <th></th>
                    </tr>
                    <tr>
                        <td>Number of Posts</td>
                        <td>60436</td>
                    </tr>
                    <tr>
                        <td>Number of Topics</td>
                        <td>8</td>
                    </tr>
                    <tr>
                        <td>Number of Hashtags</td>
                        <td>10983</td>
                    </tr>
                    <tr>
                        <td>Most used hashtag</td>
                        <td>#travel</td>
                    </tr>
                </table>
                <center><h5>Table 1: Description of the dataset</h5></center>
        <h3><center><hr width=20%><a href="https://drive.google.com/drive/folders/1IakHwyTgaTKxZa0RqQ99TRuamhEi-8xp?usp=sharing">Link to the datasets</a><hr width=20%></h3><br></center>
        </h4>

        <hr>
        <hr>
        <br>

        <h2><span style="font-weight: bold;">System Architecture</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            Figure 3, shows the architecture of the model proposed by us. For a given Instagram post, we
first separate out the images and text data. The image data is then pre-processed and passed on
to an image classifier, which is basically a CNN such as Resnet50. The classifier is used to
predict the topic corresponding to each image which is then passed on to our text based model for
further processing. We simultaneously apply some pre-processing on the text as such as stemming
and lemmatization to use it for further downstream tasks. In order to segment the given hastags
into multiple hashtags, we utilize the Viterbi algorithm and this improves the accuracy of our
approach significantly. Our text based approach consists of 8 word embeddings trained on the 8
topics mentioned earlier respectively. The pre-processed text data along with the topic predicted
by the image classifier is then used to find the 10 most relevant hashtags for the given post based
on a cosine similarity metric. We describe in detail the methodology for each task below.<br><br></h4>
<center><img src="fig3_.jpg" width=70%><br><h5>Figure 3: Proposed architecture description</h5></center>
        <br>
        <hr>
        <hr>
        <br>
        <h2><span style="font-weight: bold;">Image Classifier</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            We train an image classifier such as Resnet-50 and Resnet-34 for the task of image classification.
We model our problem as a multi-label classification problem and leverage transfer learning to
finetune the model on our dataset. We train all our models using PyTorch and utilize the negative
cross entropy loss for training. The optimizer used for training is the Adam optimizer with an
initial learning rate of 0.01. We also leverage certain other approaches such as early stopping and
cosine annealing along with data augmentations to improve the accuracy of the model. Our task is
slightly ill posed as the labels provided for training is not annotated by humans. Rather, the labels
are based on the topics provided by the user for an Instagram posts and are bound to have several
mis-classifications.
        </h4><br>


        <hr>
        <hr><br>


        <h2><span style="font-weight: bold;">Text Classifier</span></h2>
        <ul>
            <li>
                <h3><span style="font-weight: bold;">Pre-processing & Hashtag Segmentation</span></h3>
                <center><img src="fig4_.jpg" width=70%><br><h5>Figure 4: Hashtag segmentation algorithm</h5></center>
                <h4 style="text-align: justify; font-weight: normal;">
                As a part of pre-processing, we have converted the input text into lower case and then removed
                special characters, emojis and other non-english words. There can be multiple words present in the
                input sentence without space (formed as one word) so, we have used segmentation to extract the
                words from a single word that can help us predicting hashtags.
                Hashtags are complex structures with multiple words combined into single words. (eg : #pi-
                coftheday). For training purposes, we segment such complex hashtags into multiple words using the
                ‘Viterbi algorithm’ to find the most likely sequence of hidden states. An example is “foodnature” =
                [ “food”, “nature”]. We also apply stemming after hashtag segmentation as further pre-processing.
                The Viterbi algorithm used by us has been shown in Figure 4 above.
                </h4>
            </li>
            <li>
                <h3><span style="font-weight: bold;">Training</span></h3>
                    <h4 style="text-align: justify; font-weight: normal;">
                    We have trained a word embedding approach like Word2Vec [3] and Glove embeddings [5] on our
                    corpus and used distance measures such as cosine similarity to get the most relevant hashtags. As
                    one of the baselines, we also aim to implement a simple topic modelling based approach (LDA) for
                    the task of hashtag generation. The approach is detailed below.
                    A sample post representing the typical posts on Instagram has been shown in Figure 5.
                            </h4><br><br>
                            <center><img src="fig2.jpg" width=25%><br><h5>Figure 5: Shows lack of sufficient text</h5></center>
                            <h4 style="text-align: justify; font-weight: normal;">Typically a post consists of the following:
                    <ul><li>A photo</li>
                    <li>A caption of 2-3 lines</li>
                    <li>A bunch of hashtags</li>
                    </ul><br>
                        From above it is clear that the amount of text present is very less and therefore using only
                    the text for training our model won’t be enough. We tackle the above problem using below 2
                    approaches:<br><br>
                    <ul>
                    <li>Assuming the hashtags that appear together are semantically related to each other we can also
                    convert them to text and include them in our training corpus. Using the above assumption
                    we prepare our corpus by first preprocessing (removing non-english texts, emoticons, etc.)
                    and then training two different models on the prepared corpus.<br><br>
                    <ul><li>Word2Vec: Using 30 epochs and window size of 5 we trained the model to create 300
                    dimensional word embeddings.</li>
                    <li>GloVe: Using 30 epochs for training we trained the model to create 300 dimensional
                    word embeddings.</li></ul></li><br>
                    <li>To better capture the semantic relationship between words we use pre-trained word embed-
                    dings for a corpus size of 400K words trained on wikipedia data. We train our word2Vec
                    model with our corpus on top of it which helps to capture the context of the instagram
                    corpus. Therefore our vocabulary is able to cover more words and is able to capture both
                    the generic and insta-specific meaning of a word.</li>
                    </ul><br>
                    Based on the above mentioned points, we propose three main methods of training as specified
                    below:<br><br>
                    <ol>
                    <li><b>Topic based Glove Model</b>:- We have used glove embedding to find similarity between
                    words. Here for each topic, we have one model that is trained on its corresponding topic’s
                    corpus.</li><br>
                    <li><b>Topic based Word2vec Model</b>:- We have used word2vec embedding to find similarity
                    between words. Here for each topic we have one model that is trained on corresponding
                    topic’s corpus plus pre trained wikipedia glove embedding.</li><br>
                    <li><b>Global word embeddings</b>:- We have trained one global model irrespective of topic that is
                    trained on complete corpus plus pre trained wikipedia glove embedding.</li>
                    </ol>
                </h4>
            </li><br>
    <hr>
    <hr><br>
    <h2><span style="font-weight: bold;">Evaluation Mechanism</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            Evaluation mechanisms like precision and recall (with exact keyword match) cannot be used for the
            problem of hashtag prediction because even though the predicted hashtag might be contextually
            similar to the ground truth hashtag but the above mechanisms will label it as misclassification.
            Therefore for our problem we use a ‘Similarity Confusion Matrix’ for finding the accuracy (based
            on K-means clustering algorithm).
            For each post, we keep predicted hashtags as rows and ground truth hashtags as the columns
            and then calculate the cosine similarity for each pair. For each predicted hashtag (row), we consider
            the ground truth hashtag (column) which has the maximum cosine similarity with our predicted
            hashtag. If the cosine similarity score corresponding to this maximum value is above a certain
            threshold (0.5), then it is considered to be a correctly predicted hashtag 3 . We perform the above
            for all of our predicted hashtags (10) and compute the number of correctly predicted hashtags out
            of 10. This provides us the accuracy of our predictions for each post.
            For each ground truth hashtag (column) we take the maximum row and claim that the predicted
            hashtag corresponding to that row is the closest (or most appropriate) to the original hashtag. And
            if the similarity metric is above a certain threshold, it is considered as correctly predicted. A brief
            example of the computation of our metric for a given post is provided in Figure 6.<hr width=30%>
            <center><img src="fig6.jpg" width=70%><br><h5>Figure 6: A sample image showing the computation of our metric for a given post.<br>Here, the
columns are the ground truth hashtags and the rows are the hashtags predicted by us.<br>As the
similarity between the ground truth hashtags and our predicted hashtags<br>is more than 0.5 for 9 out
of 10 hashtags, the accuracy for this post is 0.9</h5></center><hr width=30%><br>
        For the image classifier, we simply use accuracy as a metric to evaluate the performance of our
        approach. We do not use any other metric such as F1 score or precision and recall as our dataset is
        pretty balanced with equal number of images for each category.
        We propose 2 baselines and evaluate our approach against them and an oracle model:<br><br>
        <ol>
            <li><b>Top K Baseline</b>:- For each post, we first predict the topic based on the image or using the
            ground truth itself and return the top 10 hashtags for this topic</li><br>
            <li><b>Global word2vec</b>:- We train a single text model on the whole corpuse and then predict the
            top 10 most similar words based on the sentence embeddings. This baseline does not use any
            image data or topic specific corpus</li><br>
            <li><b>Pretrained Wikipedia word2vec embeddings</b>:- In this model, we use word embeddings
            pretrained model on a Wikipedia corpus and finetune on our dataset. This is another form of
            global embedding approach.</li><br>
            <li><b>Topic based word2vec & Glove embeddings</b>:- We train 8 separate glove and word2vec
            embeddings on the corpus for each topic. During test time, we first predict the topic for a
            post using the image classifier and then predict hashtags using the particular topic specific
            word embeddings.</li><br>
            <li><b>Oracle</b>:- Similar to the above approach but we assume access to the ground truth topics for
            a given post.</li>
        </ol>
        </h4>
        <br>



        <hr>
        <hr><br>
    <h2><span style="font-weight: bold;">Results</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            Table 2 shows the performance of our image classifier on the given dataset. The resnet-50 model
            performs the best followed by the resnet-34 model. The model clearly performs well on our semi-
            labelled dataset and gets an accuracy of upto 73%. This is a pretty decent accuracy for the given
            data as a single image might actually belong to multiple classes as well.<br><br><br>
            <table>
                    <tr>
                        <th>Model</th>
                        <th>Accuracy</th>
                    </tr>
                    <tr>
                        <td>resnet-34</td>
                        <td>71.2</td>
                    </tr>
                    <tr>
                        <td>resnet-50</td>
                        <td>72.8</td>
                    </tr>
            </table>
            <center><h5>Table 2: Image classifier results</h5></center><br>
            Table 3 shows the performance of our approach with respect to the baselines and the oracle
            approach. We also evaluate the importance of our hashtag segmentation step by computing the
            accuracy with and without the hashtag segmentation as shown in Table 4.<br><br><br>
            <table>
                    <tr>
                        <th>Model</th>
                        <th>Accuracy</th>
                    </tr>
                    <tr>
                        <td>Top K Baseline</td>
                        <td>41.829%</td>
                    </tr>
                    <tr>
                        <td>Word2Vec + Wikipedia glove embeddings</td>
                        <td>87.558%</td>
                    </tr>
                    <tr>
                        <td>Global model (Word2Vec)</td>
                        <td>63.040%</td>
                    </tr>
                    <tr>
                        <td>Global model (Glove)</td>
                        <td>74.18%</td>
                    </tr>
                    <tr>
                        <td>Our approach (Glove with topics)</td>
                        <td>96.172%</td>
                    </tr>
                    <tr>
                        <td>Oracle</td>
                        <td>96.525%</td>
                    </tr>
            </table>
            <center><h5>Table 4: Ablation study on the importance of hashtag segmentation</h5></center>
        </h4><hr width="20%">
            <center><h3><a href="https://github.com/talsperre/IRE-hashtag-generation">Link to the repository (Github)</a><br></h3>
            <h3><a href="https://rebrand.ly/ire-hashtags-generation-video">Link to the video (YouTube)</a><br></h3></center><br>
        <hr>
        <hr><br>


        <h2><span style="font-weight: bold;">Analysis of results</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            From the above tables, we can see that the image classifier performs well for the given dataset. We
also create a confusion matrix to find the topics in which the classifier gets confused the most as
shown in Figure 7(a). We find that the model confuses between certain classes a bit more such as
travel and architecture, travel and nature and so on. We believe that modelling our problem as
a multi-label task instead of a simple multi-class & single label task would improve the accuracy
further. We can also notice that the word embeddings have semantics associated with them. On
applying LDA on the word embeddings followed by TSNE, we can see that the hashtags related to
a given topic get clustered together as can be seen in Figure 7(b).<br><br>
        <center><img src="fig7a.jpg" width=35%><br><h5>Figure 7(a): Confusion matrix of image classifier,</h5>
        <img src="fig7b.jpg" width=35% border=1><br><h5>Figure 7(b): Clustering of word embeddings</h5></center>
        <br>

        From the results of Table 4, we can see that the use of segmentation in our approach improves
        the accuracy of our model significantly. This shows that, there exists several hashtags in our dataset
        which contain two or more sub-hashtags which provide more context when segmented out.
        We also show the sample hashtags predicted by our approach for a given post in Figure 8.
        From this image, we can see that the hashtags predicted by our model are very relevant and are
        semantically related. Thus, the model can be deployed to predict Instagram hashtags in the wild.
        From the Table 3, we can also see that the use of predictions made by the image classifier instead
        of the ground truth (oracle) does not significantly reduce the performance of our approach. This
        implies that our image classifier performs pretty decently.<br><br>
        <center><img src="fig3.jpg" width=25%><br><h5>Figure 8: <b>Predicted Hashtags</b>: #view #mountain #world #amazing #ig</h5></center>

        </h4><br>

        <hr><hr>
        <br>
        <h2><span style="font-weight: bold;">Conclusion</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            We propose a hierarchical approach to predict hashtags for a multi-modal data such as Instagram
            posts. We collect over 60,000 posts and train an image and text based approach for the same.
            We propose a new metric for computing the relevance of the predicted hashtags and provide an
            algorithm to compute the same. We then evaluate our model against some baselines as well as an oracle approach and show that the approach performs significantly better than the baselines and
            only slightly worse than the oracle.

        </h4><br>
        
        <hr><hr>
        <br>
        <h2><span style="font-weight: bold;">References</span></h2>
        <h4 style="text-align: justify; font-weight: normal;">
            <ol>
                <li>Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim. Attend to you: Personalized image
                captioning with context sequence memory networks. In Proceedings of the IEEE Conference
                on Computer Vision and Pattern Recognition, pages 895–903, 2017.</li>
                <li>Emily Denton, Jason Weston, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. User con-
                ditional hashtag prediction for images. In Proceedings of the 21th ACM SIGKDD international
                conference on knowledge discovery and data mining, pages 1731–1740. ACM, 2015.</li>
                <li>Shivam Gaur. Generation of a short narrative caption for an image using the suggested hashtag.
                In 2019 IEEE 35th International Conference on Data Engineering Workshops (ICDEW), pages
                331–337. IEEE, 2019.</li>
                <li>Fréderic Godin, Viktor Slavkovikj, Wesley De Neve, Benjamin Schrauwen, and Rik Van de
                Walle. Using topic models for twitter hashtag recommendation. In Proceedings of the 22nd
                International Conference on World Wide Web, pages 593–596. ACM, 2013.</li>
                <li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
                recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
                pages 770–778, 2016.</li>
                <li>Zongyang Ma, Aixin Sun, Quan Yuan, and Gao Cong. Tagging your tweets: A probabilistic
                modeling of hashtag annotation in twitter. In Proceedings of the 23rd ACM International
                Conference on Conference on Information and Knowledge Management, pages 999–1008. ACM,
                2014.</li>
                <li>Rishabh Mehrotra, Scott Sanner, Wray Buntine, and Lexing Xie. Improving lda topic models
                for microblogs via tweet pooling and automatic labeling. In Proceedings of the 36th international
                ACM SIGIR conference on Research and development in information retrieval, pages 889–892.
                ACM, 2013.</li>
                <li>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word
                representations in vector space. arXiv preprint arXiv:1301.3781, 2013.</li>
                <li>Eriko Otsuka, Scott A Wallace, and David Chiu. A hashtag recommendation system for twitter
                data streams. Computational social networks, 3(1):3, 2016.</li>
                <li>Shreyash Pandey and Abhijeet Phatak. Cs229 project report.</li>
                <li>Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim. Towards personalized image
                captioning via multimodal memory networks. IEEE transactions on pattern analysis and
                machine intelligence, 41(4):999–1012, 2018.</li>
                <li>Minseok Park, Hanxiang Li, and Junmo Kim. Harrison: A benchmark on hashtag recommen-
                dation for real-world images in social networks, 2016.</li>
                <li>Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
                representation. In Proceedings of the 2014 conference on empirical methods in natural language
                processing (EMNLP), pages 1532–1543, 2014.</li>
                <li>Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas
                Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. arXiv
                preprint arXiv:1503.01817, 2015.</li>
                <li>Yue Wang, Jing Li, Irwin King, Michael R Lyu, and Shuming Shi. Microblog hashtag generation
                via encoding conversation contexts. arXiv preprint arXiv:1905.07584, 2019.</li>
                <li>Qi Zhang, Jiawen Wang, Haoran Huang, Xuanjing Huang, and Yeyun Gong. Hashtag recom-
                mendation for multimodal microblog using co-attention network. In IJCAI, pages 3420–3426,
                2017</li>
            </ol>
        </h4>
        <hr>



    </div>



    <!-- <div class="qual-results">
        <h3 style="font-weight: bold;">Qualitative Results</h3>
        <h4 style="font-weight: bold;">Sample Video sequence in KITTI Tracking dataset</h4>
        <video width="500" height="280" controls>
            <source src="/videos/infer-kitti-no-audio.mp4" type="video/mp4" frameborder="0" allowfullscreen>
        </video>
        <h4 style="font-weight: bold;">Sample transfer in Cityscapes dataset</h4>
        <video width="500" height="280" controls>
            <source src="/videos/infer-cscapes-no-audio.mp4" type="video/mp4" frameborder="0" allowfullscreen>
        </video>
        <h4 style="font-weight: bold;">Sample transfer in Oxford RobotCar dataset</h4>
        <video width="500" height="280" controls>
            <source src="/videos/infer-robotcar-no-audio.mp4" type="video/mp4" frameborder="0" allowfullscreen>
        </video>
        <h4 style="font-weight: bold;">Trajectories plotted on the Occupancy Grids for KITTI dataset</h4>
        <img src="/images/qual_kitti.png" style="width:1260px;height:280px;">
    </div>    
    <hr>
    <h3 style="font-weight: bold;">Some more qualitative plots</h4>
    <div class="image-container">
        <div class="imgContainer">
            <img src="/images/val-cv-1-0007.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-2-0001.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-3-0008.png" height="240" width="320"/>
        </div>
        <div>
            <img src="/images/val-cv-3-0009.png" height="240" width="320"/>
       </div>       
    </div>
    <br> 
    <div class="image-container">
        <div class="imgContainer">
            <img src="/images/val-cv-0-0013.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-1-0006.png" height="240" width="320"/>
        </div>
        <div class="imgContainer">
            <img src="/images/val-cv-4-0010.png" height="240" width="320"/>
        </div>
        <div>
            <img src="/images/val-cv-1-0004.png" height="240" width="320"/>
       </div>       
    </div>
    <br> 
    <hr>
    <div class="results">
        <h3 style="font-weight: bold;">Results</h3>
        <p style="text-align:justify; font-size: 14px;">We perform 5 fold cross validation while training and evaluating on KITTI. The error for each split is given in the below 
            tables. We report the pixel loss (L2 norm) here and to get the error in m, multiply the values by 0.25. The average loss for a given model is computed as the 
            weighted mean of the losses of each individual split. The weights used here are the number of sequences in each split.
        </p>
        <h4 style="font-weight: bold;">INFER-Skip results with all channels (pixel loss)</h4>
        <span style="text-align: center;">
            <table>
                    <tr>
                        <th></th>
                        <th>1s</th>
                        <th>2s</th>
                        <th>3s</th>
                        <th>4s</th>
                        <th>Num Sequences</th>
                    </tr>
                    <tr>
                        <td>Split 0</td>
                        <td>2.2</td>
                        <td>2.91</td>
                        <td>3.68</td>
                        <td>5.29</td>
                        <td>16</td>
                    </tr>
                    <tr>
                        <td>Split 1</td>
                        <td>1.97</td>
                        <td>2.72</td>
                        <td>2.75</td>
                        <td>2.75</td>
                        <td>19</td>
                    </tr>
                    <tr>
                        <td>Split 2</td>
                        <td>2.54</td>
                        <td>3.3</td>
                        <td>4.15</td>
                        <td>5.74</td>
                        <td>28</td>
                    </tr>
                    <tr>
                        <td>Split 3</td>
                        <td>2.32</td>
                        <td>3.16</td>
                        <td>3.86</td>
                        <td>4.82</td>
                        <td>22</td>
                    </tr>
                    <tr>
                        <td>Split 4</td>
                        <td>2.06</td>
                        <td>2.9</td>
                        <td>4.13</td>
                        <td>5.79</td>
                        <td>16</td>
                    </tr>                    
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">INFER results with all channels (pixel loss)</h4>        
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>2.82</td>
                    <td>3.8</td>
                    <td>5.56</td>
                    <td>7.9</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>2.57</td>
                    <td>3.55</td>
                    <td>4.31</td>
                    <td>4.76</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.4</td>
                    <td>3.98</td>
                    <td>5.64</td>
                    <td>7.75</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.99</td>
                    <td>3.94</td>
                    <td>4.49</td>
                    <td>5.43</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>1.44</td>
                    <td>1.7</td>
                    <td>2.65</td>
                    <td>4.19</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">Baseline (pixel loss)</h4>        
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>3.17</td>
                    <td>5.04</td>
                    <td>6.47</td>
                    <td>8.16</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>1.66</td>
                    <td>2.80</td>
                    <td>3.71</td>
                    <td>4.55</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>4.07</td>
                    <td>7.45</td>
                    <td>9.68</td>
                    <td>11.13</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>3.50</td>
                    <td>5.08</td>
                    <td>6.29</td>
                    <td>7.67</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>2.32</td>
                    <td>2.75</td>
                    <td>4.16</td>
                    <td>6.15</td>
                    <td>16</td>
                </tr>
            </table>
        </span>        
        <br>
        <h4 style="font-weight: bold;">INFER w/o road (pixel loss)</h4>
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>3.08</td>
                    <td>4.68</td>
                    <td>6.03</td>
                    <td>8.23</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>3.76</td>
                    <td>6.08</td>
                    <td>9.73</td>
                    <td>13.83</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.65</td>
                    <td>4.76</td>
                    <td>6.94</td>
                    <td>9.21</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.32</td>
                    <td>4.16</td>
                    <td>6.49</td>
                    <td>9.18</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>2.53</td>
                    <td>4.58</td>
                    <td>6.84</td>
                    <td>9.61</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">INFER w/o lane (pixel loss)</h4>
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>2.34</td>
                    <td>3.42</td>
                    <td>4.85</td>
                    <td>7.32</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>1.73</td>
                    <td>2.42</td>
                    <td>2.81</td>
                    <td>3.23</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.97</td>
                    <td>3.76</td>
                    <td>4.46</td>
                    <td>5.5</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.66</td>
                    <td>3.67</td>
                    <td>4.27</td>
                    <td>5.05</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>1.29</td>
                    <td>1.4</td>
                    <td>2.01</td>
                    <td>3.06</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>
        <h4 style="font-weight: bold;">INFER w/o obstacles (pixel loss)</h4>
        <span style="text-align: center">
            <table>
                <tr>
                    <th></th>
                    <th>1s</th>
                    <th>2s</th>
                    <th>3s</th>
                    <th>4s</th>
                    <th>Num Sequences</th>
                </tr>
                <tr>
                    <td>Split 0</td>
                    <td>2.08</td>
                    <td>2.85</td>
                    <td>3.55</td>
                    <td>4.48</td>
                    <td>16</td>
                </tr>
                <tr>
                    <td>Split 1</td>
                    <td>2.42</td>
                    <td>3.25</td>
                    <td>3.38</td>
                    <td>3.23</td>
                    <td>19</td>
                </tr>
                <tr>
                    <td>Split 2</td>
                    <td>2.20</td>
                    <td>3.47</td>
                    <td>4.56</td>
                    <td>5.56</td>
                    <td>28</td>
                </tr>
                <tr>
                    <td>Split 3</td>
                    <td>2.65</td>
                    <td>3.84></td>
                    <td>4.75</td>
                    <td>6.03</td>
                    <td>22</td>
                </tr>
                <tr>
                    <td>Split 4</td>
                    <td>1.39</td>
                    <td>2.23</td>
                    <td>3.42</td>
                    <td>5.17</td>
                    <td>16</td>
                </tr>
            </table>
        </span>
        <br>        
    </div> -->
</div><!-- /.blurb -->

        
        </div><!-- /.container -->
        <!-- <footer>
            <ul>
                <li>Email: <a href="mailto:s.shashank2401@gmail.com">s.shashank2401@gmail.com</a></li>
                <li><a href="https://github.com/talsperre">github.com/talsperre</a></li>
            </ul>
        </footer>                            -->
    </body>
</html>